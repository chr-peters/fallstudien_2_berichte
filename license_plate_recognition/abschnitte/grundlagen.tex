\section{Grundlagen}
\label{sec:grundlagen}

\subsection{Neuronale Netze}
\label{sec:neuronale-netze}

Neuronale Netze sind eine Modellklasse, welche zur L\"osung des
bereits ausf\"uhrlich
in~\cite{statistical_learning} beschriebenen Problems des statistischen
Lernens eingesetzt werden k\"onnen.
In dieser Problemsituation wird angenommen, dass sich der Zusammenhang zwischen
beobachtbaren Pr\"adiktorvariablen $X_1, ..., X_p$, welche sich durch
einen Vektor $X = (X_1, ..., X_p)$ zusammenfassen lassen, und einer
Zielvariable $Y$ durch eine Funktion $f^*$ mit $Y = f^*(X) + \epsilon$
modellieren l\"asst. Hierbei kann $\epsilon$ als eine zuf\"allige
St\"orgr\"o{\ss}e angesehen werden, die hier im weiteren Verlauf aber
keine wichtige Rolle spielt.
Das Ziel von Neuronalen Netzen ist es, die unbekannte Funktion $f^*$
zu approximieren.

Die folgenden Erkl\"arungen zum Aufbau und zum Training neuronaler Netze
st\"utzen sich wesentlich auf~\cite{Goodfellow-et-al-2016} und sind hier
auf das grundlegendste reduziert worden.
Obwohl es der Name \textit{neuronale} Netze suggeriert, wird auch hier
genau wie in~\cite{Goodfellow-et-al-2016} ebenfalls auf jegliche
biologische Motivation verzichtet, damit nicht der f\"alschliche Eindruck
entstehen kann, dass es sich bei neuronalen Netzen um Modelle von echten
biologischen Gehirnen handeln k\"onnte.
Das Ziel von neuronalen Netzen ist es viel eher, unbekannte Funktionen
anhand von Trainingsdaten zu approximieren und die Ergebnisse dann auf
ungesehene Daten zu generalisieren.

\subsubsection{Aufbau}

Wie in~\ref{sec:neuronale-netze} angedeutet, definiert ein neuronales Netz
also eine Abbildung $f$, welche den Zusammenhang zwischen einer Eingabe $X$
und einer Ausgabe $Y$ approximieren soll.
Eine Hauptcharakteristik von neuronalen Netzen ist es, dass die Funktion $f$
durch die Verkettung weiterer Funktionen gebildet wird.
Ist ein neuronales Netz beispielsweise durch
$f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))$ gegeben, so setzt es sich aus der
Verkettung der einzelnen Funktionen $f^{(1)}$, $f^{(2)}$ und
$f^{(3)}$ zusammen. Wie genau diese Funktionen aussehen k\"onnen, soll an dieser
Stelle bewusst ersteinmal offen bleiben.

Solche Ketten von Funktionen k\"onnen gut durch
azyklische Graphen beschrieben werden. Hierbei wird jedes Zwischenergebnis
durch einen Knoten repr\"asentiert, jede Kante zwischen zwei Knoten beschreibt
die Operation, die von einen Ergebnis zum n\"achsten gef\"uhrt hat.
Der Beispielgraph zu $f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))$ ist in
Abbildung~\ref{fig:einfacher-graph} zu sehen.

\begin{figure}[h]
    \centering
    \includegraphics[height=0.3\textheight]{abbildungen/basic_network_graph}
    \caption{Verkettung dreier Funktionen dargestellt als azyklischer Graph.}
    \label{fig:einfacher-graph}
\end{figure}

Im Kontext von neuronalen Netzen wird jede der Funktionen
$f^{(1)}$, $f^{(2)}$ und $f^{(3)}$ auch als eine \textbf{Schicht} im
neuronalen Netz bezeichnet.
Da $f^{(1)}$ und $f^{(2)}$ im Inneren des Netzwerks liegen, bezeichnet man
diese Schichten auch als \textbf{versteckte Schichten}.
Die Gesamtanzahl der Schichten wird als die \textbf{Tiefe} des neuronalen
Netzes bezeichnet.

Zusammenfassend l\"asst sich also sagen, dass sich ein neuronales Netz als
eine Verkettung beliebiger Funktionen auffassen l\"asst, welche auf
eine Eingabegr\"o{\ss}e $X$ angewendet werden.
Wie genau diese Funktionen, also die Schichten des neuronalen Netzes,
aussehen k\"onnen, wird nun im n\"achsten
Abschnitt beschrieben.

\subsubsection{Schichten}

Eine Schicht eines neuronalen Netzes ist eine Funktion $f$, die eine
Eingabe $x \in \mathbb{R}^n$ auf eine Ausgabe $f(x) \in \mathbb{R}^m$
abbildet. H\"aufig hat $f$ dabei die folgende Form:
\begin{equation}
    f(x) = g(Wx + b)
\end{equation}
$W \in \mathbb{R}^{m \times n}$ ist hierbei eine sogenannte Gewichtsmatrix,
die die Eingabe $x$ der Schicht linear transformiert.
$b \in \mathbb{R}^m$ wird auch Bias genannt und wird auf das Ergebnis der
Multiplikation von $W$ mit $x$ addiert.
Die Funktion $g: \mathbb{R}^m \rightarrow \mathbb{R}^m$ hei{\ss}t
Aktivierungsfunktion.

Oftmals ist es so, dass Aktivierungsfunktionen elementweise auf das
Resultat von $Wx + b$ angewendet werden. Hierzu kann man sich eine
Funktion $\Phi: \mathbb{R} \rightarrow \mathbb{R}$ definieren und dann
$g_i(u) = \Phi(u_i), \  i=1,...,m$ setzen. Hierbei beschreibt $g_i$
das $i$-te Element der vektorwertigen Funktion $g$ und $u_i$ das $i$-te
Element des Vektors $Wx + b$.

Die Parameter $W$ und $b$ einer Schicht k\"onnen w\"ahrend der Trainingsphase
des neuronalen Netzwerks optimiert werden. Dieses Vorgehen wird in
Abschnitt~\ref{sec:training} n\"aher beschrieben.
Die Aktivierungsfunktion einer Schicht hingegen ist ein sogenannter
\textbf{Hyperparameter} des Netzwerks. Dies bedeutet, dass dieser
Parameter vor dem Training vom Anwender spezifiziert werden muss.

\subsubsection{Aktivierungsfunktionen}

Im Folgenden werden ausschlie{\ss}lich elementweise Aktivierungsfunktionen
$\Phi: \mathbb{R} \rightarrow \mathbb{R}$ betrachtet,
da diese die gr\"o{\ss}te praktische Relevanz haben.
Zwei sehr h\"aufig eingesetzte Aktivierungsfunktionen, die auch in diesem
Projekt verwendet wurden, sind die \textbf{Rectified Linear Unit (ReLU)}
Funktion und die \textbf{Sigmoid} Funktion.
Beide werden im Folgenden kurz beschrieben.

\paragraph{ReLU}

Die ReLU Funktion ist nach~\cite{Goodfellow-et-al-2016} quasi eine
Standardempfehlung f\"ur die Aktivierungsfunktion in den
versteckten Schichten moderner tiefer neuronaler Netze.
Sie ist durch folgenden Ausdruck gegeben:
\begin{equation*}
    \Phi_\text{ReLU}(x) = \max \{ 0, x \}
\end{equation*}
Wird diese Funktion elementweise auf einen Vektor angewendet, so werden
alle negativen Elemente auf Null gesetzt. Die restlichen Elemente bleiben
unber\"uhrt.

\paragraph{Sigmoid} Die Sigmoid Funktion ist durch folgenden Ausdruck gegeben:
\begin{equation*}
    \Phi_\text{Sigmoid}(x) = \frac{1}{1 + \exp{(-x)}}
\end{equation*}

\subsubsection{Training}
\label{sec:training}

\subsubsection{Convolutional Neural Networks}

\subsection{OpenCV}

\subsection{Tesseract}